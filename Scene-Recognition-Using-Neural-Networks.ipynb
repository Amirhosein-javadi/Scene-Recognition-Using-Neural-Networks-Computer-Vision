{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY9F68BqvTil"
      },
      "source": [
        "اکسترکت کردن تصاویر"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8trWsLEpPVOW"
      },
      "source": [
        "!unzip '/content/drive/MyDrive/Data.zip'\n",
        "!unzip '/content/drive/MyDrive/Augmented_Data.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQFwwl0bvbOe"
      },
      "source": [
        "زیاد کردن دیتای test و train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IAUXEBrZT8i"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import skimage\n",
        "import glob\n",
        "import math \n",
        "\n",
        "def Rotate(img,teta):\n",
        "    R = np.zeros([3,3])\n",
        "    R[2][2] =1\n",
        "    R[0][0] = math.cos(teta)\n",
        "    R[1][1] = math.cos(teta)\n",
        "    R[0][1] = -math.sin(teta)\n",
        "    R[1][0] = math.sin(teta)\n",
        "    warp_im = cv2.warpPerspective(img,R,(img.shape[1],img.shape[0]))\n",
        "    return warp_im\n",
        "\n",
        "def Enhancement(img,y):\n",
        "    enhancement_im = np.zeros_like(img)\n",
        "    enhancement_im = 255 * (img/255)**y\n",
        "    return enhancement_im\n",
        "\n",
        "def Zoom(img,scale):\n",
        "    row,col = img.shape\n",
        "    zoom_im = np.zeros_like(img)\n",
        "    if scale<1 :\n",
        "       smaller = cv2.resize(img,(0, 0),fx=scale,fy=scale,interpolation=cv2.INTER_AREA) \n",
        "       s_row,s_col = smaller.shape\n",
        "       zoom_im [row//2-s_row//2:row//2-s_row//2+s_row,col//2-s_col//2:col//2-s_col//2+s_col] = smaller \n",
        "    elif scale>1 :\n",
        "       bigger = cv2.resize(img,(0, 0),fx=scale,fy=scale,interpolation=cv2.INTER_LINEAR) \n",
        "       s_row,s_col = bigger.shape\n",
        "       zoom_im = bigger[s_row//2-row//2:s_row//2-row//2+row,s_col//2-col//2:s_col//2-col//2+col]\n",
        "    return zoom_im\n",
        "\n",
        "def Augmentation(img,folder,counter):\n",
        "    file = folder + f'{counter}.jpg'\n",
        "    cv2.imwrite(file,cv2.resize(img,(227, 227),interpolation=cv2.INTER_AREA)) \n",
        "    counter += 1\n",
        "    Flip = cv2.flip(img, 1)\n",
        "    file = folder + f'{counter}.jpg'\n",
        "    cv2.imwrite(file,cv2.resize(Flip,(227, 227),interpolation=cv2.INTER_AREA)) \n",
        "    counter += 1    \n",
        "    # Rotation\n",
        "    teta = (2/180)*math.pi\n",
        "    Warp_Im = Rotate(img,teta)\n",
        "    file = folder + f'{counter}.jpg'\n",
        "    cv2.imwrite(file,cv2.resize(Warp_Im,(227, 227),interpolation=cv2.INTER_AREA)) \n",
        "    counter += 1\n",
        "    teta = (4/180)*math.pi\n",
        "    Warp_Im = Rotate(img,teta)\n",
        "    file = folder + f'{counter}.jpg'\n",
        "    cv2.imwrite(file,cv2.resize(Warp_Im,(227, 227),interpolation=cv2.INTER_AREA)) \n",
        "    counter += 1\n",
        "    teta = (-2/180)*math.pi\n",
        "    Warp_Im = Rotate(img,teta)\n",
        "    file = folder + f'{counter}.jpg'\n",
        "    cv2.imwrite(file,cv2.resize(Warp_Im,(227, 227),interpolation=cv2.INTER_AREA)) \n",
        "    counter += 1\n",
        "    teta = (-4/180)*math.pi\n",
        "    Warp_Im = Rotate(img,teta)\n",
        "    file = folder + f'{counter}.jpg'\n",
        "    cv2.imwrite(file,cv2.resize(Warp_Im,(227, 227),interpolation=cv2.INTER_AREA)) \n",
        "    counter += 1   \n",
        "    # Rotation  Flip  \n",
        "    teta = (2/180)*math.pi\n",
        "    Warp_Im = Rotate(Flip,teta)\n",
        "    file = folder + f'{counter}.jpg'\n",
        "    cv2.imwrite(file,cv2.resize(Warp_Im,(227, 227),interpolation=cv2.INTER_AREA)) \n",
        "    counter += 1\n",
        "    teta = (4/180)*math.pi\n",
        "    Warp_Im = Rotate(Flip,teta)\n",
        "    file = folder + f'{counter}.jpg'\n",
        "    cv2.imwrite(file,cv2.resize(Warp_Im,(227, 227),interpolation=cv2.INTER_AREA)) \n",
        "    counter += 1\n",
        "    teta = (-2/180)*math.pi\n",
        "    Warp_Im = Rotate(Flip,teta)\n",
        "    file = folder + f'{counter}.jpg'\n",
        "    cv2.imwrite(file,cv2.resize(Warp_Im,(227, 227),interpolation=cv2.INTER_AREA)) \n",
        "    counter += 1\n",
        "    teta = (-4/180)*math.pi\n",
        "    Warp_Im = Rotate(Flip,teta)\n",
        "    file = folder + f'{counter}.jpg'\n",
        "    cv2.imwrite(file,cv2.resize(Warp_Im,(227, 227),interpolation=cv2.INTER_AREA)) \n",
        "    counter += 1  \n",
        "    # Darkening\n",
        "    Enhancement_Im = Enhancement(img,1.1)\n",
        "    file = folder + f'{counter}.jpg'\n",
        "    cv2.imwrite(file,cv2.resize(Enhancement_Im,(227, 227),interpolation=cv2.INTER_AREA)) \n",
        "    counter += 1       \n",
        "    Enhancement_Im = Enhancement(img,1.2)\n",
        "    file = folder + f'{counter}.jpg'\n",
        "    cv2.imwrite(file,cv2.resize(Enhancement_Im,(227, 227),interpolation=cv2.INTER_AREA)) \n",
        "    counter += 1 \n",
        "    # Brightning\n",
        "    Enhancement_Im = Enhancement(img,0.9)\n",
        "    file = folder + f'{counter}.jpg'\n",
        "    cv2.imwrite(file,cv2.resize(Enhancement_Im,(227, 227),interpolation=cv2.INTER_AREA)) \n",
        "    counter += 1    \n",
        "    Enhancement_Im = Enhancement(img,0.8)\n",
        "    file = folder + f'{counter}.jpg'\n",
        "    cv2.imwrite(file,cv2.resize(Enhancement_Im,(227, 227),interpolation=cv2.INTER_AREA)) \n",
        "    counter += 1  \n",
        "    # Zoom_in\n",
        "    Zoom_Im = Zoom(img,1.1)\n",
        "    file = folder + f'{counter}.jpg'\n",
        "    cv2.imwrite(file,cv2.resize(Zoom_Im,(227, 227),interpolation=cv2.INTER_AREA))     \n",
        "    counter += 1     \n",
        "    Zoom_Im = Zoom(img,1.2)\n",
        "    file = folder + f'{counter}.jpg'\n",
        "    cv2.imwrite(file,cv2.resize(Zoom_Im,(227, 227),interpolation=cv2.INTER_AREA))     \n",
        "    counter += 1   \n",
        "    # Zoom_out\n",
        "    Zoom_Im = Zoom(img,0.9)\n",
        "    file = folder + f'{counter}.jpg'\n",
        "    cv2.imwrite(file,cv2.resize(Zoom_Im,(227, 227),interpolation=cv2.INTER_AREA))     \n",
        "    counter += 1     \n",
        "    Zoom_Im = Zoom(img,0.8)\n",
        "    file = folder + f'{counter}.jpg'\n",
        "    cv2.imwrite(file,cv2.resize(Zoom_Im,(227, 227),interpolation=cv2.INTER_AREA))     \n",
        "    counter += 1       \n",
        "    return counter\n",
        "\n",
        "\n",
        "Data_Type = {0:\"Test\",\n",
        "             1:\"Train\"}\n",
        "\n",
        "Data_Class={0:\"Bedroom\",\n",
        "            1:\"Coast\",\n",
        "            2:\"Forest\",\n",
        "            3:\"Highway\",\n",
        "            4:\"Industrial\",\n",
        "            5:\"Inside_City\",\n",
        "            6:\"Kitchen\",\n",
        "            7:\"Livingroom\",\n",
        "            8:\"Mountain\",\n",
        "            9:\"Office\",\n",
        "            10:\"Open_Country\",\n",
        "            11:\"Store\",\n",
        "            12:\"Street\",\n",
        "            13:\"Suburb\",\n",
        "            14:\"Tall_Building\"}   \n",
        "\n",
        "counter = 0\n",
        "for i in range(2):\n",
        "    for j in range(15):\n",
        "        Source_Folder = '/content/Data/' + Data_Type[i] + '/' + Data_Class[j]\n",
        "        Target_Folder = '/content/Augmented_Data/' + Data_Type[i] + '/' + Data_Class[j]+ \"/\"\n",
        "        Files = glob.glob(f'{Source_Folder}/*.jpg')\n",
        "        for Pics in Files:\n",
        "            Img = cv2.imread(Pics,0)\n",
        "            counter = Augmentation(Img,Target_Folder,counter)\n",
        "        counter = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKNtaQHUvmhn"
      },
      "source": [
        "قسمت اول"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSEXGQKRQoY6"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0],True)\n",
        "import numpy as np\n",
        "from keras import initializers\n",
        "from keras import layers,callbacks\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Input, Dense, Activation,BatchNormalization, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import metrics\n",
        "from keras.metrics import SparseTopKCategoricalAccuracy \n",
        "import keras.backend as K\n",
        "K.set_image_data_format(\"channels_last\")\n",
        "import glob \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def True_val(File):\n",
        "  counter = 0 ;\n",
        "  true_pred = np.zeros([1,1])\n",
        "  folders = glob.glob(f'{File}/*')\n",
        "  for folder in folders:\n",
        "    num_of_classes = np.size(glob.glob(f'{folder}/*.jpg'))\n",
        "    val = np.ones([num_of_classes,1])*counter\n",
        "    true_pred = np.concatenate((true_pred,val), axis=0)\n",
        "    counter += 1\n",
        "  true_pred = np.delete(true_pred,0,axis=0)\n",
        "  return true_pred\n",
        "\n",
        "def plot_func(y1,y2,max_iter,plt_file,plt_type,plt_title):\n",
        "  x = np.array(range(1,max_iter+1,jump))\n",
        "  plt.plot(x, y1, label = f'Test {plt_type}' , color='red', marker='o', markerfacecolor='red', markersize=5)\n",
        "  plt.plot(x, y2, label = f'Train {plt_type}', color='blue' , marker='o', markerfacecolor='blue', markersize=5)\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel(f'{plt_type}')\n",
        "  plt.title(f'{plt_title}')\n",
        "  if plt_type == 'Accuracy':\n",
        "    plt.ylim(0, 1)\n",
        "  plt.legend()\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "  plt.savefig(plt_file)\n",
        "  return\n",
        "\n",
        "def AlexNet():\n",
        "  tf.config.experimental.list_logical_devices('GPU')\n",
        "  tf.debugging.set_log_device_placement(True)\n",
        "  with tf.device(tf.test.gpu_device_name()):\n",
        "      X_input = Input((227,227,3))\n",
        "      # Conv layer 1\n",
        "      X = Conv2D(96,(11,11), strides=4, name=\"Conv0\", kernel_initializer=initializers.GlorotUniform())(X_input)\n",
        "      X = BatchNormalization(axis=3, name=\"Bn0\")(X)\n",
        "      X = Activation('relu')(X)\n",
        "      X = MaxPooling2D((4,4), strides=4, name='Max0')(X)\n",
        "      # X = Dropout(0.5)(X)\n",
        "      # Fully connected layer 1\n",
        "      X = Flatten()(X)\n",
        "      X = Dense(4096, activation='relu', name=\"Fc0\", kernel_initializer=initializers.GlorotUniform())(X)\n",
        "      X = Dropout(0.5)(X)\n",
        "      # Fully connected layer 2\n",
        "      X = Dense(15,activation='softmax', name='Fc1', kernel_initializer=initializers.GlorotUniform())(X)\n",
        "      opt = SGD(learning_rate=0.001 , momentum=0.97)\n",
        "      model = Model(inputs=X_input,outputs=X, name='AlexNet')\n",
        "      model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "      return model\n",
        "\n",
        "alex = AlexNet()\n",
        "Train_Datagen = ImageDataGenerator(rescale=1./255)\n",
        "Train_Path = '/content/Augmented_Data/Train'\n",
        "Train = Train_Datagen.flow_from_directory(Train_Path, target_size=(227,227), class_mode='categorical', batch_size=256)\t\n",
        "Test_Datagen = ImageDataGenerator(rescale=1./255)\n",
        "Test_Path = '/content/Augmented_Data/Test'\n",
        "Test = Test_Datagen.flow_from_directory(Test_Path, target_size=(227,227), class_mode='categorical', batch_size=256)\n",
        "y_pred_test = True_val('/content/Augmented_Data/Test/')\n",
        "y_pred_train = True_val('/content/Augmented_Data/Train/')\n",
        "max_iter = 41\n",
        "jump = 3\n",
        "Test_Loss_Top1 = np.zeros((max_iter-1)//jump+1)\n",
        "Test_Accuracy_Top1 = np.zeros((max_iter-1)//jump+1)\n",
        "Train_Loss_Top1 = np.zeros((max_iter-1)//jump+1)\n",
        "Train_Accuracy_Top1 = np.zeros((max_iter-1)//jump+1)\n",
        "Test_Accuracy_Top5 = np.zeros((max_iter-1)//jump+1)\n",
        "Train_Accuracy_Top5 = np.zeros((max_iter-1)//jump+1)\n",
        "m = SparseTopKCategoricalAccuracy(k=5)\n",
        "\n",
        "for k in range(1,max_iter+1,jump):\n",
        "  with tf.device(tf.test.gpu_device_name()):\n",
        "    alex.fit(Train,epochs=k)\n",
        "    Train_Top1 = alex.evaluate(Train)\n",
        "    Train_Loss_Top1[(k-1)//jump]=Train_Top1[0]\n",
        "    Train_Accuracy_Top1[(k-1)//jump]=Train_Top1[1]\n",
        "    Test_Top1 = alex.evaluate(Test)\n",
        "    Test_Loss_Top1[(k-1)//jump]=Test_Top1[0]\n",
        "    Test_Accuracy_Top1[(k-1)//jump]=Test_Top1[1]\n",
        "    #Train_Top5 = alex.predict(Train)\n",
        "    #m.reset_states()\n",
        "    #m.update_state(y_pred_train,Train_Top5) \n",
        "    #Train_Accuracy_Top5[(k-1)//jump] = m.result().numpy()\n",
        "    #Test_Top5 = alex.predict(Test)\n",
        "    #m.reset_states()\n",
        "    #m.update_state(y_pred_test,Test_Top5) \n",
        "    #Test_Accuracy_Top5[(k-1)//jump] = m.result().numpy()    \n",
        "    #print(Test_Accuracy_Top1)\n",
        "    #print(Test_Accuracy_Top5)\n",
        "    #print(Test_Loss_Top1)\n",
        "    #print(Train_Accuracy_Top1)\n",
        "    #print(Train_Accuracy_Top5)\n",
        "    #print(Train_Loss_Top1)\n",
        "\n",
        "plot_func(Test_Accuracy_Top1,Train_Accuracy_Top1,max_iter,'/content/Accuracy_top1.png','Accuracy',\"Top-1\")\n",
        "#plot_func(Test_Accuracy_Top5,Train_Accuracy_Top5,max_iter,'/content/Accuracy_top5.png','Accuracy',\"Top-5\")\n",
        "plot_func(Test_Loss_Top1,Train_Loss_Top1,max_iter,'/content/Loss_top1.png','Loss',\"Top-1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzjUnQgyvuuc"
      },
      "source": [
        "قسمت دوم"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05NZXtMK-yZm"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0],True)\n",
        "import numpy as np\n",
        "from keras import initializers\n",
        "from keras import layers,callbacks\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Input, Dense, Activation,BatchNormalization, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import metrics\n",
        "from keras.metrics import SparseTopKCategoricalAccuracy \n",
        "import keras.backend as K\n",
        "K.set_image_data_format(\"channels_last\")\n",
        "import glob \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def True_val(File):\n",
        "  counter = 0 ;\n",
        "  true_pred = np.zeros([1,1])\n",
        "  folders = glob.glob(f'{File}/*')\n",
        "  for folder in folders:\n",
        "    num_of_classes = np.size(glob.glob(f'{folder}/*.jpg'))\n",
        "    val = np.ones([num_of_classes,1])*counter\n",
        "    true_pred = np.concatenate((true_pred,val), axis=0)\n",
        "    counter += 1\n",
        "  true_pred = np.delete(true_pred,0,axis=0)\n",
        "  return true_pred\n",
        "\n",
        "def plot_func(y1,y2,max_iter,plt_file,plt_type,plt_title):\n",
        "  x = np.array(range(1,max_iter+1,jump))\n",
        "  plt.plot(x, y1, label = f'Test {plt_type}' , color='red', marker='o', markerfacecolor='red', markersize=5)\n",
        "  plt.plot(x, y2, label = f'Train {plt_type}', color='blue' , marker='o', markerfacecolor='blue', markersize=5)\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel(f'{plt_type}')\n",
        "  plt.title(f'{plt_title}')\n",
        "  if plt_type == 'Accuracy':\n",
        "    plt.ylim(0, 1)\n",
        "  plt.legend()\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "  plt.savefig(plt_file)\n",
        "  return\n",
        "\n",
        "def AlexNet():\n",
        "  tf.config.experimental.list_logical_devices('GPU')\n",
        "  tf.debugging.set_log_device_placement(True)\n",
        "  with tf.device(tf.test.gpu_device_name()):\n",
        "      X_input = Input((227,227,3))\n",
        "      # Conv layer 0\n",
        "      X = Conv2D(96,(11,11), strides=4, name=\"Conv0\", kernel_initializer=initializers.GlorotUniform())(X_input)\n",
        "      X = BatchNormalization(axis=3, name=\"Bn0\")(X)\n",
        "      X = Activation('relu')(X)\n",
        "      X = MaxPooling2D((3,3), strides=2, name='max0')(X)\n",
        "      X = Dropout(0.5)(X)\n",
        "      # Conv layer 1\n",
        "      X = Conv2D(256,(5,5), padding='same', name='Conv1', kernel_initializer=initializers.GlorotUniform())(X)\n",
        "      X = BatchNormalization(axis=3, name='Bn1')(X)\n",
        "      X = Activation('relu')(X)\n",
        "      X = MaxPooling2D((3,3), strides=2, name='Max1')(X)\n",
        "      X = Dropout(0.5)(X)\n",
        "      # Conv layer 2\n",
        "      X = Conv2D(256, (3,3), padding='same', name='Conv2', kernel_initializer=initializers.GlorotUniform())(X)\n",
        "      X = BatchNormalization(axis=3, name='Bn2')(X)\n",
        "      X = Activation('relu')(X)\n",
        "      X = MaxPooling2D((2,2), strides=2, name='Max2')(X)\n",
        "      X = Dropout(0.5)(X)\n",
        "      # Fully connected layer 0 \n",
        "      X = Flatten()(X)\n",
        "      X = Dense(4096, activation='relu', name=\"Fc0\", kernel_initializer=initializers.GlorotUniform())(X)\n",
        "      X = Dropout(0.5)(X) \n",
        "      # Fully connected layer 1 \n",
        "      X = Dense(4096, activation='relu', name=\"Fc1\", kernel_initializer=initializers.GlorotUniform())(X)\n",
        "      X = Dropout(0.5)(X) \n",
        "      # Fully connected layer 2 \n",
        "      X = Dense(15, activation='softmax', name=\"Fc2\", kernel_initializer=initializers.GlorotUniform())(X)\n",
        "      opt=SGD(learning_rate=0.001 , momentum=0.97)\n",
        "      model = Model(inputs=X_input,outputs=X, name='AlexNet')\n",
        "      model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "      return model\n",
        "\n",
        "alex = AlexNet()\n",
        "Train_Datagen = ImageDataGenerator(rescale=1./255)\n",
        "Train_Path = '/content/Augmented_Data/Train'\n",
        "Train = Train_Datagen.flow_from_directory(Train_Path, target_size=(227,227), class_mode='categorical', batch_size=256)\t\n",
        "Test_Datagen = ImageDataGenerator(rescale=1./255)\n",
        "Test_Path = '/content/Augmented_Data/Test'\n",
        "Test = Test_Datagen.flow_from_directory(Test_Path, target_size=(227,227), class_mode='categorical', batch_size=256)\n",
        "y_pred_test = True_val('/content/Augmented_Data/Test/')\n",
        "y_pred_train = True_val('/content/Augmented_Data/Train/')\n",
        "max_iter = 41\n",
        "jump = 3\n",
        "Test_Loss_Top1 = np.zeros((max_iter-1)//jump+1)\n",
        "Test_Accuracy_Top1 = np.zeros((max_iter-1)//jump+1)\n",
        "Train_Loss_Top1 = np.zeros((max_iter-1)//jump+1)\n",
        "Train_Accuracy_Top1 = np.zeros((max_iter-1)//jump+1)\n",
        "Test_Accuracy_Top5 = np.zeros((max_iter-1)//jump+1)\n",
        "Train_Accuracy_Top5 = np.zeros((max_iter-1)//jump+1)\n",
        "m = SparseTopKCategoricalAccuracy(k=5)\n",
        "\n",
        "m = SparseTopKCategoricalAccuracy(k=5)\n",
        "\n",
        "for k in range(1,max_iter+1,jump):\n",
        "  with tf.device(tf.test.gpu_device_name()):\n",
        "    alex.fit(Train,epochs=k)\n",
        "    Train_Top1 = alex.evaluate(Train)\n",
        "    Train_Loss_Top1[(k-1)//jump]=Train_Top1[0]\n",
        "    Train_Accuracy_Top1[(k-1)//jump]=Train_Top1[1]\n",
        "    Test_Top1 = alex.evaluate(Test)\n",
        "    Test_Loss_Top1[(k-1)//jump]=Test_Top1[0]\n",
        "    Test_Accuracy_Top1[(k-1)//jump]=Test_Top1[1]\n",
        "    #Train_Top5 = alex.predict(Train)\n",
        "    #m.reset_states()\n",
        "    #m.update_state(y_pred_train,Train_Top5) \n",
        "    #Train_Accuracy_Top5[(k-1)//jump] = m.result().numpy()\n",
        "    #Test_Top5 = alex.predict(Test)\n",
        "    #m.reset_states()\n",
        "    #m.update_state(y_pred_test,Test_Top5) \n",
        "    #Test_Accuracy_Top5[(k-1)//jump] = m.result().numpy()    \n",
        "    #print(Test_Accuracy_Top1)\n",
        "    #print(Test_Accuracy_Top5)\n",
        "    #print(Test_Loss_Top1)\n",
        "    #print(Train_Accuracy_Top1)\n",
        "    #print(Train_Accuracy_Top5)\n",
        "    #print(Train_Loss_Top1)\n",
        "\n",
        "plot_func(Test_Accuracy_Top1,Train_Accuracy_Top1,max_iter,'/content/Accuracy_top1.png','Accuracy',\"Top-1\")\n",
        "#plot_func(Test_Accuracy_Top5,Train_Accuracy_Top5,max_iter,'/content/Accuracy_top5.png','Accuracy',\"Top-5\")\n",
        "plot_func(Test_Loss_Top1,Train_Loss_Top1,max_iter,'/content/Loss_top1.png','Loss',\"Top-1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L15F9Zpgvy-O"
      },
      "source": [
        "قسمت سوم"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9Jc5lVwHAZs"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0],True)\n",
        "import numpy as np\n",
        "from keras import initializers\n",
        "from keras import layers,callbacks\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Input, Dense, Activation,BatchNormalization, Flatten, Conv2D, MaxPooling2D, Dropout\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import metrics\n",
        "from keras.metrics import SparseTopKCategoricalAccuracy \n",
        "import keras.backend as K\n",
        "K.set_image_data_format(\"channels_last\")\n",
        "import glob \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def True_val(File):\n",
        "  counter = 0 ;\n",
        "  true_pred = np.zeros([1,1])\n",
        "  folders = glob.glob(f'{File}/*')\n",
        "  for folder in folders:\n",
        "    num_of_classes = np.size(glob.glob(f'{folder}/*.jpg'))\n",
        "    val = np.ones([num_of_classes,1])*counter\n",
        "    true_pred = np.concatenate((true_pred,val), axis=0)\n",
        "    counter += 1\n",
        "  true_pred = np.delete(true_pred,0,axis=0)\n",
        "  return true_pred\n",
        "\n",
        "def plot_func(y1,y2,max_iter,plt_file,plt_type,plt_title):\n",
        "  x = np.array(range(1,max_iter+1,jump))\n",
        "  plt.plot(x, y1, label = f'Test {plt_type}' , color='red', marker='o', markerfacecolor='red', markersize=5)\n",
        "  plt.plot(x, y2, label = f'Train {plt_type}', color='blue' , marker='o', markerfacecolor='blue', markersize=5)\n",
        "  plt.xlabel('x - epoch')\n",
        "  plt.ylabel(f'{plt_type} - axis')\n",
        "  plt.title(f'{plt_title}')\n",
        "  plt.legend()\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "  plt.savefig(plt_file)\n",
        "\n",
        "def AlexNet():\n",
        "  tf.config.experimental.list_logical_devices('GPU')\n",
        "  tf.debugging.set_log_device_placement(True)\n",
        "  with tf.device(tf.test.gpu_device_name()):\n",
        "      X_input = Input((227,227,3))\n",
        "      # Conv layer 0\n",
        "      X = Conv2D(96,(11,11), strides=4, name=\"Conv0\", kernel_initializer=initializers.GlorotUniform())(X_input)\n",
        "      X = BatchNormalization(axis=3, name=\"Bn0\")(X)\n",
        "      X = Activation('relu')(X)\n",
        "      X = MaxPooling2D((3,3), strides=2, name='Max0')(X)\n",
        "      X = Dropout(0.5)(X)\n",
        "      # Conv layer 1\n",
        "      X = Conv2D(256,(5,5), padding='same', name='Conv1', kernel_initializer=initializers.GlorotUniform())(X)\n",
        "      X = BatchNormalization(axis = 3 ,name='Bn1')(X)\n",
        "      X = Activation('relu')(X)\n",
        "      X = MaxPooling2D((3,3), strides=2, name='Max1')(X)\n",
        "      X = Dropout(0.5)(X)\n",
        "      # Conv layer 2\n",
        "      X = Conv2D(384, (3,3), padding='same', name='Conv2', kernel_initializer=initializers.GlorotUniform())(X)\n",
        "      X = BatchNormalization(axis=3, name='Bn2')(X)\n",
        "      X = Activation('relu')(X)\n",
        "      # Conv layer 3\n",
        "      X = Conv2D(384, (3,3), padding='same', name='Conv3', kernel_initializer=initializers.GlorotUniform())(X)\n",
        "      X = BatchNormalization(axis=3, name='Bn3')(X)\n",
        "      X = Activation('relu')(X)\n",
        "      # Conv layer 4\n",
        "      X = Conv2D(256, (3,3), padding='same', name='Conv4')(X)\n",
        "      X = BatchNormalization(axis=3, name='Bn4')(X)\n",
        "      X = Activation('relu')(X) \n",
        "      X = MaxPooling2D((3,3), strides=2, name='Max4')(X)\n",
        "      X = Dropout(0.5)(X)    \n",
        "      # Fully connected layer 0 \n",
        "      X = Flatten()(X)\n",
        "      X = Dense(4096, activation='relu', name=\"Fc0\", kernel_initializer=initializers.GlorotUniform())(X)\n",
        "      X = Dropout(0.5)(X) \n",
        "      # Fully connected layer 1 \n",
        "      X = Dense(4096, activation='relu', name=\"Fc1\", kernel_initializer=initializers.GlorotUniform())(X)\n",
        "      X = Dropout(0.5)(X) \n",
        "      # Fully connected layer 2 \n",
        "      X = Dense(15, activation='softmax', name=\"Fc2\", kernel_initializer=initializers.GlorotUniform())(X)\n",
        "      model = Model(inputs=X_input,outputs=X, name='AlexNet')\n",
        "      opt=SGD(learning_rate=0.001 , momentum=0.97)\n",
        "      model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "      return model\n",
        "\n",
        "alex = AlexNet()\n",
        "Train_Datagen = ImageDataGenerator(rescale=1./255)\n",
        "Train_Path = '/content/Augmented_Data/Train'\n",
        "Train = Train_Datagen.flow_from_directory(Train_Path, target_size=(227,227), class_mode='categorical', batch_size=256)\t\n",
        "Test_Datagen = ImageDataGenerator(rescale=1./255)\n",
        "Test_Path = '/content/Augmented_Data/Test'\n",
        "Test = Test_Datagen.flow_from_directory(Test_Path, target_size=(227,227), class_mode='categorical', batch_size=256)\n",
        "y_pred_test = True_val('/content/Augmented_Data/Test/')\n",
        "y_pred_train = True_val('/content/Augmented_Data/Train/')\n",
        "max_iter = 41\n",
        "jump = 3\n",
        "Test_Loss_Top1 = np.zeros((max_iter-1)//jump+1)\n",
        "Test_Accuracy_Top1 = np.zeros((max_iter-1)//jump+1)\n",
        "Train_Loss_Top1 = np.zeros((max_iter-1)//jump+1)\n",
        "Train_Accuracy_Top1 = np.zeros((max_iter-1)//jump+1)\n",
        "Test_Accuracy_Top5 = np.zeros((max_iter-1)//jump+1)\n",
        "Train_Accuracy_Top5 = np.zeros((max_iter-1)//jump+1)\n",
        "m = SparseTopKCategoricalAccuracy(k=5)\n",
        "\n",
        "m = SparseTopKCategoricalAccuracy(k=5)\n",
        "\n",
        "for k in range(1,max_iter+1,jump):\n",
        "  with tf.device(tf.test.gpu_device_name()):\n",
        "    alex.fit(Train,epochs=k)\n",
        "    Train_Top1 = alex.evaluate(Train)\n",
        "    Train_Loss_Top1[(k-1)//jump]=Train_Top1[0]\n",
        "    Train_Accuracy_Top1[(k-1)//jump]=Train_Top1[1]\n",
        "    Test_Top1 = alex.evaluate(Test)\n",
        "    Test_Loss_Top1[(k-1)//jump]=Test_Top1[0]\n",
        "    Test_Accuracy_Top1[(k-1)//jump]=Test_Top1[1]\n",
        "    #Train_Top5 = alex.predict(Train)\n",
        "    #m.reset_states()\n",
        "    #m.update_state(y_pred_train,Train_Top5) \n",
        "    #Train_Accuracy_Top5[(k-1)//jump] = m.result().numpy()\n",
        "    #Test_Top5 = alex.predict(Test)\n",
        "    #m.reset_states()\n",
        "    #m.update_state(y_pred_test,Test_Top5) \n",
        "    #Test_Accuracy_Top5[(k-1)//jump] = m.result().numpy()    \n",
        "    #print(Test_Accuracy_Top1)\n",
        "    #print(Test_Accuracy_Top5)\n",
        "    #print(Test_Loss_Top1)\n",
        "    #print(Train_Accuracy_Top1)\n",
        "    #print(Train_Accuracy_Top5)\n",
        "    #print(Train_Loss_Top1)\n",
        "\n",
        "plot_func(Test_Accuracy_Top1,Train_Accuracy_Top1,max_iter,'/content/Accuracy_top1.png','Accuracy',\"Top-1\")\n",
        "#plot_func(Test_Accuracy_Top5,Train_Accuracy_Top5,max_iter,'/content/Accuracy_top5.png','Accuracy',\"Top-5\")\n",
        "plot_func(Test_Loss_Top1,Train_Loss_Top1,max_iter,'/content/Loss_top1.png','Loss',\"Top-1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65n3JRVHtthG"
      },
      "source": [
        "قسمت چهارم\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR8RiyzEf-vY"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_func(y1,y2,max_iter,plt_file,plt_type,plt_title):\n",
        "  x = np.array(range(1,max_iter+1,jump))\n",
        "  plt.plot(x, y1, label = f'Test {plt_type}' , color='red', marker='o', markerfacecolor='red', markersize=5)\n",
        "  plt.plot(x, y2, label = f'Train {plt_type}', color='blue' , marker='o', markerfacecolor='blue', markersize=5)\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel(f'{plt_type}')\n",
        "  plt.title(f'{plt_title}')\n",
        "  if plt_type == 'Accuracy':\n",
        "    plt.ylim(0, 1)\n",
        "  plt.legend()\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "  plt.savefig(plt_file)\n",
        "  return\n",
        "\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "Last_layer = nn.Linear(in_features=4096, out_features=15, bias=True)\n",
        "FC = list(alexnet.classifier.children())\n",
        "del FC[-1]\n",
        "FC.append(Last_layer)\n",
        "Conv = list(alexnet.features.children())\n",
        "Flat = nn.Flatten()\n",
        "Conv.append(Flat)\n",
        "model = Conv + FC\n",
        "alexnet = nn.Sequential(*model)\n",
        "for p in alexnet[0:-1].parameters():\n",
        "  p.requires_grad = False\n",
        "for p in alexnet[-1].parameters():\n",
        "  p.requires_grad = True\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "alexnet = alexnet.to(device)\n",
        "Opt = optim.SGD(alexnet[-1].parameters(), lr=0.001, momentum=0.5)\n",
        "Criterion = nn.CrossEntropyLoss() \n",
        "Train_path = \"/content/Data/Train/\"\n",
        "Test_path = \"/content/Data/Test/\"\n",
        "Transform_Img = transforms.Compose([transforms.Resize((227,227)),transforms.ToTensor()])\n",
        "\n",
        "Trainset  = torchvision.datasets.ImageFolder(root=Train_path,transform=Transform_Img)\n",
        "Trainloader  = torch.utils.data.DataLoader(Trainset, batch_size=4, shuffle=False,  num_workers=2)\n",
        "Testset  = torchvision.datasets.ImageFolder(root=Test_path,transform=Transform_Img)\n",
        "Testloader   = torch.utils.data.DataLoader(Testset , batch_size=4, shuffle=False, num_workers=2) \n",
        "max_iter = 41\n",
        "jump = 3\n",
        "Test_Loss_Top1 = np.zeros((max_iter-1)//jump+1)\n",
        "Test_Accuracy_Top1 = np.zeros((max_iter-1)//jump+1)\n",
        "Test_Accuracy_Top5 = np.zeros((max_iter-1)//jump+1)\n",
        "Train_Loss_Top1 = np.zeros((max_iter-1)//jump+1)\n",
        "Train_Accuracy_Top1 = np.zeros((max_iter-1)//jump+1)\n",
        "Train_Accuracy_Top5 = np.zeros((max_iter-1)//jump+1)\n",
        "\n",
        "for k in range(1,max_iter,jump):\n",
        "  alexnet = nn.Sequential(*model)\n",
        "  alexnet = alexnet.to(device)\n",
        "  for epochs in range(k):\n",
        "    Sum = 0.0\n",
        "    for i, data in enumerate(Trainloader, 0):\n",
        "      inputs, labels = data\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "      Opt.zero_grad()\n",
        "      outputs = alexnet(inputs)\n",
        "      Loss = Criterion(outputs, labels)\n",
        "      Loss.backward()\n",
        "      Opt.step()\n",
        "      Sum += Loss.item()\n",
        "  Total = 0\n",
        "  with torch.no_grad():\n",
        "    for data in Testloader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        outputs = alexnet(inputs)\n",
        "        Top5 = outputs.data.topk(5,dim=1).indices\n",
        "        Loss = Criterion(outputs, labels)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        Total += labels.size(0)\n",
        "        Test_Accuracy_Top1[(k-1)//jump] += (predicted == labels).sum().item()\n",
        "        for i in range(Top5.shape[0]):\n",
        "          Test_Accuracy_Top5[(k-1)//jump] += (labels[i] in Top5[i])*1      \n",
        "        Test_Loss_Top1[(k-1)//jump] += Loss\n",
        "    Test_Accuracy_Top1[(k-1)//jump] /= Total\n",
        "    Test_Accuracy_Top5[(k-1)//jump] /= Total\n",
        "    Test_Loss_Top1[(k-1)//jump] /= Total\n",
        "  Total = 0  \n",
        "  with torch.no_grad():\n",
        "    for data in Trainloader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        outputs = alexnet(inputs)\n",
        "        Top5 = outputs.data.topk(5,dim=1).indices\n",
        "        Loss = Criterion(outputs, labels)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        Total += labels.size(0)\n",
        "        Train_Accuracy_Top1[(k-1)//jump] += (predicted == labels).sum().item()\n",
        "        for i in range(Top5.shape[0]):\n",
        "          Train_Accuracy_Top5[(k-1)//jump] += (labels[i] in Top5[i])*1     \n",
        "        Train_Loss_Top1[(k-1)//jump] += Loss\n",
        "    Train_Accuracy_Top1[(k-1)//jump] /= Total\n",
        "    Train_Accuracy_Top5[(k-1)//jump] /= Total\n",
        "    Train_Loss_Top1[(k-1)//jump] /= Total\n",
        "    #print(Test_Accuracy_Top1)\n",
        "    #print(Test_Accuracy_Top5)\n",
        "    #print(Test_Loss_Top1)\n",
        "    #print(Train_Accuracy_Top1)\n",
        "    #print(Train_Accuracy_Top5)\n",
        "    #print(Train_Loss_Top1)\n",
        "\n",
        "plot_func(Test_Accuracy_Top1,Train_Accuracy_Top1,max_iter,'/content/Accuracy_top1.png','Accuracy',\"Top-1\")\n",
        "plot_func(Test_Accuracy_Top5,Train_Accuracy_Top5,max_iter,'/content/Accuracy_top5.png','Accuracy',\"Top-5\")\n",
        "plot_func(Test_Loss_Top1,Train_Loss_Top1,max_iter,'/content/Loss_top1.png','Loss',\"Top-1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyH7R87mupCg"
      },
      "source": [
        "قسمت پنجم"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqDLurCNTyAC"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_func(y1,y2,max_iter,plt_file,plt_type,plt_title):\n",
        "  x = np.array(range(1,max_iter+1,jump))\n",
        "  plt.plot(x, y1, label = f'Test {plt_type}' , color='red', marker='o', markerfacecolor='red', markersize=5)\n",
        "  plt.plot(x, y2, label = f'Train {plt_type}', color='blue' , marker='o', markerfacecolor='blue', markersize=5)\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel(f'{plt_type}')\n",
        "  plt.title(f'{plt_title}')\n",
        "  if plt_type == 'Accuracy':\n",
        "    plt.ylim(0, 1)\n",
        "  plt.legend()\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "  plt.savefig(plt_file)\n",
        "  return\n",
        "\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "Last_layer = nn.Linear(in_features=4096, out_features=15, bias=True)\n",
        "FC = list(alexnet.classifier.children())\n",
        "del FC[-1]\n",
        "FC.append(Last_layer)\n",
        "Conv = list(alexnet.features.children())\n",
        "Flat = nn.Flatten()\n",
        "Conv.append(Flat)\n",
        "model = Conv + FC\n",
        "alexnet = nn.Sequential(*model)\n",
        "for p in alexnet.parameters():\n",
        "  p.requires_grad = True\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "alexnet = alexnet.to(device)\n",
        "Opt = optim.SGD(alexnet.parameters(), lr=0.0005, momentum=0.5)\n",
        "Criterion = nn.CrossEntropyLoss()\n",
        "Train_path = \"/content/Data/Train/\"\n",
        "Test_path = \"/content/Data/Test/\"\n",
        "Transform_Img = transforms.Compose([transforms.Resize((227,227)),transforms.ToTensor()])\n",
        "Trainset  = torchvision.datasets.ImageFolder(root=Train_path,transform=Transform_Img)\n",
        "Trainloader  = torch.utils.data.DataLoader(Trainset, batch_size=4, shuffle=False,  num_workers=2)\n",
        "Testset  = torchvision.datasets.ImageFolder(root=Test_path,transform=Transform_Img)\n",
        "Testloader   = torch.utils.data.DataLoader(Testset , batch_size=4, shuffle=False, num_workers=2)\n",
        "max_iter = 41\n",
        "jump = 3\n",
        "Test_Loss_Top1 = np.zeros((max_iter-1)//jump+1)\n",
        "Test_Accuracy_Top1 = np.zeros((max_iter-1)//jump+1)\n",
        "Test_Accuracy_Top5 = np.zeros((max_iter-1)//jump+1)\n",
        "Train_Loss_Top1 = np.zeros((max_iter-1)//jump+1)\n",
        "Train_Accuracy_Top1 = np.zeros((max_iter-1)//jump+1)\n",
        "Train_Accuracy_Top5 = np.zeros((max_iter-1)//jump+1)\n",
        "\n",
        "for k in range(1,max_iter,jump):\n",
        "  alexnet = nn.Sequential(*model)\n",
        "  alexnet = alexnet.to(device)\n",
        "  for epochs in range(k):\n",
        "    Sum = 0.0\n",
        "    for i, data in enumerate(Trainloader, 0):\n",
        "      inputs, labels = data\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "      Opt.zero_grad()\n",
        "      outputs = alexnet(inputs)\n",
        "      Loss = Criterion(outputs, labels)\n",
        "      Loss.backward()\n",
        "      Opt.step()\n",
        "      Sum += Loss.item()\n",
        "  Total = 0\n",
        "  with torch.no_grad():\n",
        "    for data in Testloader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        outputs = alexnet(inputs)\n",
        "        Top5 = outputs.data.topk(5,dim=1).indices\n",
        "        Loss = Criterion(outputs, labels)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        Total += labels.size(0)\n",
        "        Test_Accuracy_Top1[(k-1)//jump] += (predicted == labels).sum().item()\n",
        "        for i in range(Top5.shape[0]):\n",
        "          Test_Accuracy_Top5[(k-1)//jump] += (labels[i] in Top5[i])*1      \n",
        "        Test_Loss_Top1[(k-1)//jump] += Loss\n",
        "    Test_Accuracy_Top1[(k-1)//jump] /= Total\n",
        "    Test_Accuracy_Top5[(k-1)//jump] /= Total\n",
        "    Test_Loss_Top1[(k-1)//jump] /= Total\n",
        "  Total = 0  \n",
        "  with torch.no_grad():\n",
        "    for data in Trainloader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        outputs = alexnet(inputs)\n",
        "        Top5 = outputs.data.topk(5,dim=1).indices\n",
        "        Loss = Criterion(outputs, labels)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        Total += labels.size(0)\n",
        "        Train_Accuracy_Top1[(k-1)//jump] += (predicted == labels).sum().item()\n",
        "        for i in range(Top5.shape[0]):\n",
        "          Train_Accuracy_Top5[(k-1)//jump] += (labels[i] in Top5[i])*1     \n",
        "        Train_Loss_Top1[(k-1)//jump] += Loss\n",
        "    Train_Accuracy_Top1[(k-1)//jump] /= Total\n",
        "    Train_Accuracy_Top5[(k-1)//jump] /= Total\n",
        "    Train_Loss_Top1[(k-1)//jump] /= Total\n",
        "    #print(Test_Accuracy_Top1)\n",
        "    #print(Test_Accuracy_Top5)\n",
        "    #print(Test_Loss_Top1)\n",
        "    #print(Train_Accuracy_Top1)\n",
        "    #print(Train_Accuracy_Top5)\n",
        "    #print(Train_Loss_Top1)\n",
        "\n",
        "plot_func(Test_Accuracy_Top1,Train_Accuracy_Top1,max_iter,'/content/Accuracy_top1.png','Accuracy',\"Top-1\")\n",
        "plot_func(Test_Accuracy_Top5,Train_Accuracy_Top5,max_iter,'/content/Accuracy_top5.png','Accuracy',\"Top-5\")\n",
        "plot_func(Test_Loss_Top1,Train_Loss_Top1,max_iter,'/content/Loss_top1.png','Loss',\"Top-1\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}